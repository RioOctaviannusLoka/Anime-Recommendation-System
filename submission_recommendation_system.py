# -*- coding: utf-8 -*-
"""Submission Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfOUw6nwySTvz-o-URFLM5KZpTioHTEE

# Import Library

Pertama-tama, dilakukan import untuk seluruh library yang digunakan untuk proyek ini.
"""

from google.colab import userdata
import os

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""# Data Understanding

Awalnya, diambil data terkait username dan key kaggle dari Google Colab Secrets supaya bisa mengakses kaggle untuk pengambilan dataset AnimeList Dataset.
"""

os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')
os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')

"""Selanjutnya, dilakukan pengambilan [dataset AnimeList](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database) dari platform Kaggle. Data yang diambil dalam bentuk zip kemudian akan di unzip terlebih dahulu sehingga muncul dua file csv bernama anime.csv dan rating.csv."""

!kaggle datasets download -d CooperUnion/anime-recommendations-database
!unzip anime-recommendations-database.zip

"""Kemudian, dilakukan perubahan format dataset dari csv menjadi Pandas DataFrame supaya memudahkan pengolahan dataset tersebut. DataFrame dari anime.csv disimpan dalam variabel **anime_df**, sedangkan rating.csv disimpan dalam variabel **rating_df**."""

anime_df = pd.read_csv('anime.csv')
rating_df = pd.read_csv('rating.csv')

"""Kemudian kita melihat isi 10 baris pertama dari dataset anime."""

anime_df.head(10)

"""Selanjutnya kita melihat isi 10 baris pertama dari dataset rating."""

rating_df.head(10)

"""# Exploratory Data Analysis

## Deskripsi Variabel

Variabel | Keterangan
---------|-----------
anime_id | ID unik myanimelist.net yang mengidentifikasi sebuah anime
name     | Nama lengkap anime
genre    | Daftar genre untuk anime ini yang dipisahkan dengan koma
type     | Tipe anime, seperti movie, TV, OVA, dll
episodes | Berapa banyak episode dalam acara ini (1 jika film)
rating   | Rata-rata penilaian dari 10 untuk anime ini
members  | Jumlah anggota komunitas yang berada dalam grup anime ini.

Variabel yang akan dipakai dalam proyek ini adalah `anime_id`, `name`, dan `genre`.

Kode dibawah akan menampilkan informasi tentang setiap variabel dalam dataset **anime_df**
"""

anime_df.info()

"""Dari hasil kode diatas, terlihat bahwa ada 3 variabel numerik dan 4 variabel kategorikal bertipe data object. Terlihat juga bahwa ada beberapa variabel yang memiliki null type seperti pada variabel `genre`, `type`, dan `rating`.

Variabel | Keterangan
---------|-----------
user_id  | ID pengguna
anime_id | Anime yang telah dinilai oleh pengguna ini
rating   | Penilaian dari 10 yang diberikan pengguna ini (-1 jika pengguna menontonnya tetapi tidak memberikan penilaian)

Variabel yang akan dipakai dalam proyek ini adalah `anime_id`, `user_id`, dan `rating`.

Selanjutnya akan ditampilkan informasi tentang setiap variabel dalam dataset **rating_df**
"""

rating_df.info()

"""Dari hasil kode diatas, terlihat bahwa ada 3 variabel numerik yaitu `user_id`, `anime_id`, `rating`

Selanjutnya kita akan melihat deskripsi dataset **anime_df** terkait sebaran datanya, nilai minimal, nilai maksimal, dan lain-lain.
"""

anime_df.describe(include="all")

"""Dari hasil kode diatas, bisa dilihat untuk nilai count, unique, top, freq, mean, std, min, Q1, Q2, Q3, dan max dari setiap variabel numerik pada **anime_df**.

Selanjutnya kita akan melihat deskripsi dataset **rating_df** terkait sebaran datanya, nilai minimal, nilai maksimal, dan lain-lain.
"""

rating_df.describe()

"""Dari hasil kode diatas, bisa dilihat untuk nilai count, mean, std, min, Q1, Q2, Q3, dan max dari setiap variabel numerik pada **rating_df**.

## Cek Missing Value

Pada bagian ini, kita akan mengecek apakah terdapat missing value di dataset **anime_df**
"""

anime_df.isna().sum()

"""Dari hasil kode tersebut, terlihat bahwa terdapat 62 null value pada `genre`, 25 null value pada `type`, dan 230 null value pada `rating`

Selanjutnya kita melakukan pengecekan missing value pada dataset **rating_df**
"""

rating_df.isna().sum()

"""Dari hasil kode diatas, terlihat bahwa tidak terdapat missing value pada **rating_df**

Selanjutnya kita mengecek jumlah user yang memberikan rating -1. Rating -1 dianggap sebagai null value karena rating tersebut menunjukkan bahwa pengguna menonton anime tersebut tetapi tidak memberikan penilaian/rating. Jadi seluruh rating -1 akan ditangani selayaknya null value.
"""

rating_df[rating_df['rating'] == -1].shape

"""Dari hasil kode diatas, terlihat bahwa terdapat 1476496 baris yang ratingnya -1. Seluruh baris tersebut akan ditangani nantinya selayaknya null value.

## Univariate Analysis

### Analisis Distribusi Rating Anime

Kemudian, dilakukan visualisasi data menggunakan bar chart untuk melihat bagaimana distribusi rating yang diberikan pengguna kepada anime-anime yang ada melalui **rating_df**
"""

ratings_count = rating_df['rating'].value_counts().sort_index()

plt.figure(figsize=(10,6))
sns.barplot(x=ratings_count.index, y=ratings_count.values, palette='viridis')
plt.title("Distribusi Rating Anime")
plt.xlabel("Rating")
plt.ylabel("Jumlah Anime")
plt.tight_layout()
plt.show()

"""Dari grafik diatas, terlihat bahwa user kebanyakan memberikan rating dari rentang 7 hingga 10 dengan rating 8 merupakan rating terbanyak yang diberikan oleh user kepada anime-anime yang ada.

### Analisis Genre

Selanjutnya, dilakukan pengecekan jumlah genre yang ada pada setiap anime dengan memisahkan setiap list genre pada anime yang memiliki genre lebih dari satu. Lalu kita juga mengecek apa saja genre-genre yang ada pada **anime_df**
"""

genre_series = anime_df['genre'].dropna().str.split(', ')
genre_list = [genre for sublist in genre_series for genre in sublist]
genre_counts = pd.Series(genre_list).value_counts()

print("Jumlah Genre: ", len(genre_counts))
print("Genre: ", genre_counts.index)

"""Dari output kode diatas, terlihat bahwa terdapat 43 genre dari anime-anime pada dataset **anime_df**. Terlihat juga list genre anime tersebut.

Kemudian, dilakukan visualisasi data menggunakan bar chart untuk melihat frekuensi anime dari 43 genre yang ada.
"""

plt.figure(figsize=(12,6))
sns.barplot(x=genre_counts.values, y=genre_counts.index)
plt.title("Frekuensi Anime di Setiap Genre")
plt.xlabel("Jumlah")
plt.ylabel("Genre")
plt.tight_layout()
plt.show()

"""Dari grafik diatas, dapat dilihat bahwa anime dengan genre Comedy merupakan anime yang paling banyak jumlahnya yaitu lebih dari 4000, kemudian diikuti dengan genre Action dan Adventure. Genre yang paling sedikit jumlah animenya yaitu Yaoi, Yuri, dan Josei.

### Analisis Distribusi Tipe Anime

Pada bagian ini, dilakukan visualisasi menggunakan bar chart untuk menggambarkan distribusi jumlah anime dari variabel `Type` pada **anime_df**.
"""

plt.figure(figsize=(8,5))
sns.countplot(data=anime_df, x='type', order=anime_df['type'].value_counts().index)
plt.title("Distribusi Tipe Anime")
plt.xlabel("Tipe")
plt.ylabel("Jumlah")
plt.tight_layout()
plt.show()

"""Terlihat dari grafik diatas, bahwa tipe anime yang paling dominan adalah TV, dengan jumlah lebih dari 3700 judul. Ini menandakan bahwa format serial televisi merupakan bentuk yang paling umum digunakan dalam industri anime, kemungkinan karena daya tariknya yang tinggi dalam membangun cerita panjang dan basis penggemar yang stabil. Di posisi kedua dan ketiga terdapat tipe OVA (Original Video Animation) dan Movie, masing-masing dengan jumlah sekitar 3300 dan 2300 judul. Secara keseluruhan, grafik ini memberikan gambaran bahwa produksi anime masih sangat didominasi oleh format TV, sementara tipe-tipe lain berperan sebagai pelengkap atau variasi distribusi konten.

# Data Preparation

Ditahap ini, kita melakukan persiapan terhadap data supaya memiliki bentuk yang sesuai untuk digunakan pada model serta hanya fitur-fitur penting saja yang disertakan dalam pelatihan model. Ada beberapa tahap persiapan data yang dilakukan:
1. Menangani Missing Value (Nilai Kosong)
2. Menangani Duplikasi Data
3. Melakukan Penggabungan Data
4. Menghapus Fitur yang tidak dibutuhkan
5. Mengganti Nama Kolom

## Menangani Missing Value (Nilai Kosong)

Ditahap ini, dilakukan penanganan terhadap missing value yang terdapat pada **anime_df** dan **rating_df** dengan melakukan penghapusan terhadap missing value tersebut.

Kode dibawah menunjukkan ukuran dataset sebelum dilakukan penghapusan null value pada dataset **anime_df** dan setelah dilakukan penghapusan tersebut.
"""

print("Jumlah missing value dalam setiap kolom:")
print(anime_df.isnull().sum())
print("\nUkuran dataset anime: ", anime_df.shape)

anime_df.dropna(inplace=True)
print("\nSetelah missing value dihapus")
print("Jumlah missing value dalam setiap kolom:")
print(anime_df.isnull().sum())

print("\nUkuran dataset anime: ", anime_df.shape)

"""Terlihat bahwa ukuran dataset **anime_df** setelah dilakukan penghapusan null value yaitu terdiri dari 12017 baris dan 7 kolom.

Selanjutnya dilakukan pengecekan jumlah null value pada dataset **rating_df**
"""

rating_df.isna().sum()

"""Dari kode diatas, terlihat bahwa tidak terdapat missing value sama sekali pada dataset **rating_df**

Kemudian untuk rating yang bernilai -1 pada dataset **rating_df** akan dihapus karena dianggap sebagai missing value. Rating -1 dianggap sebagai null value karena rating tersebut menunjukkan bahwa pengguna menonton anime tersebut tetapi tidak memberikan penilaian/rating
"""

rating_df = rating_df[rating_df['rating'] != -1]
rating_df

"""Terlihat bahwa setelah dilakukan penghapusan rating bernilai -1, ukuran dataset **rating_df** sekarang adalah 6.337.241 baris dan 3 kolom.

## Menangani Duplikasi Data

Ditahap ini, dilakukan penghapusan terhadap data duplikat pada dataset **anime_df** dan **rating_df**. Pertama-tama dilakukan pengecekan jumlah data duplikat pada dataset **anime_df**
"""

anime_df.duplicated().sum()

"""Dari hasil kode diatas, terlihat bahwa tidak terdapat data duplikat pada dataset **anime_df**

Selanjutnya dilakukan pengecekan jumlah data duplikat pada dataset **rating_df**
"""

print("Jumlah duplikasi data pada rating_df: ", rating_df.duplicated().sum())

"""Terlihat bahwa terdapat 1 data duplikat pada **rating_df**. Karena jumlah data duplikat hanya bernilai 1, maka data duplikat tersebut akan dihapus"""

rating_df.drop_duplicates(inplace=True)
print("Jumlah duplikasi data pada rating_df: ", rating_df.duplicated().sum())
rating_df.shape

"""Setelah data duplikat tersebut dihapus, dilakukan pengecekan ulang terhadap jumlah data duplikat pada **rating_df**. Terlihat bahwa jumlah data yang duplikat sudah tidak ada dan ukuran dataset **rating_df** setelah dihapus data duplikat tersebut adalah 6.337.240 baris data dan 3 kolom.

## Melakukan Penggabungan Data

Disini, dilakukan penggabungan dataset antara **rating_df** dan **anime_df** berdasarkan `anime_id`. Penggabungan dataset ini dilakukan secara inner, yaitu hanya anime_id yang beririsan pada **rating_df** dan **anime_df**
"""

df_ratings_anime = pd.merge(rating_df, anime_df, on='anime_id', how='inner')
df_ratings_anime

"""## Menghapus Fitur yang tidak dibutuhkan

Ditahap ini, dilakukan penghapusan fitur yang tidak dipakai untuk pelatihan model kita yaitu `type`, `episodes`, `rating_y`, `members`. Penghapusan fitur ini dilakukan dengan menggunakan method drop.
"""

df_ratings_anime.drop(['type', 'episodes', 'rating_y', 'members'], inplace=True, axis=1)
df_ratings_anime

"""## Mengganti Nama Kolom

Selanjutnya dilakukan penggantian nama kolom `rating_x` menjadi `rating` supaya memudahkan interpretasi dan pemanggilan kolom.
"""

df_ratings_anime.rename(columns={'rating_x': 'rating'}, inplace=True)
df_ratings_anime

"""# Content Based Filtering

Content-based filtering adalah metode dalam sistem rekomendasi yang memberikan rekomendasi item kepada pengguna berdasarkan kemiripan karakteristik item dengan item yang disukai atau dinilai tinggi oleh pengguna tersebut sebelumnya

## Data Preparation

Ditahap ini, ada beberapa tahap persiapan data yang dilakukan, yaitu:
1. Menghapus Data anime_id yang Duplikat
2. Pembuatan Matriks TF-IDF

### Menghapus Data anime_id yang Duplikat

Pertama-tama, dataframe yang dibuat sebelumnya dicopy terlebih dahulu ke sebuah variabel baru bernama **anime**
"""

anime = df_ratings_anime.copy()
anime.sort_values('anime_id')

"""Ditahap ini, hanya `anime_id` unik yang akan digunakan pada proses pemodelan. Oleh karena itu, dilakukan penghapusan data `anime_id` yang duplikat dengan fungsi drop_duplicates()."""

anime = anime.drop_duplicates('anime_id')
anime

"""### Pembuatan Matriks TF-IDF

Pertama-tama, dilakukan konversi data series menjadi list menggunakan fungsi tolist() dari library numpy. Data-data ini adalah data yang akan digunakan untuk pemodelan Content Based Filtering yaitu data `anime_id`, `name`, dan `genre`.
"""

anime_id = anime['anime_id'].tolist()
anime_name = anime['name'].tolist()
anime_genres = anime['genre'].tolist()

print(len(anime_id))
print(len(anime_name))
print(len(anime_genres))

"""Selanjutnya, dibuat dictionary untuk menentukan pasangan key-value pada data anime_id, anime_name, dan anime_genres yang telah disiapkan sebelumnya. Dictionary tersebut kemudian diubah menjadi sebuah dataframe bernama **anime_new**"""

anime_new = pd.DataFrame({'anime_id': anime_id, 'name': anime_name, 'genre': anime_genres})
anime_new

"""Selanjutnya dilakukan penghapusan spasi setelah koma pada kolom genre sehingga dapat mempermudah proses ekstraksi fitur menggunakan tf-idf. Kodenya adalah sebagai berikut:"""

anime_new['genre'] = anime_new['genre'].apply(lambda x: ','.join(g.strip() for g in x.split(',')) if pd.notna(x) else '')

"""Kemudian, akan digunakan TF-IDF Vectorizer untuk menemukan representasi fitur penting dari setiap genre anime. Penerapannya dilakukan dengan menggunakan fungsi tfidfvectorizer() dari library sklearn. Selanjutnya tfidf di fit menggunakan data kolom genre dan ditampilkan semua nama fitur pada tfidf tersebut."""

tfidf = TfidfVectorizer(token_pattern=r'[^,]+')
tfidf.fit(anime_new['genre'])
tfidf.get_feature_names_out()

"""Selanjutnya, dilakukan fit dan transformasi tfidf ke dalam bentuk matriks."""

tfidf_matrix = tfidf.fit_transform(anime_new['genre'])
tfidf_matrix.shape

"""Dapat dilihat dari output kode tersebut bahwa matriks dari tfidf berukuran (9892, 43). Nilai 9892 merupakan ukuran data dan 43 merupakan ukuran genre anime.

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, digunakan fungsi todense(). Kodenya sebagai berikut.
"""

tfidf_matrix.todense()

"""## Modelling

Ditahap ini, dilakukan penghitungan derajat kesamaan (similarity degree) antar anime dengan teknik cosine similarity menggunakan fungsi cosine_similarity dari library sklearn.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Selanjutnya, dibuat sebuah DataFrame yang berisi cosine similarity antara judul anime dengan judul anime lainnya. Kemudian dilihat DataFrame tersebut dalam 5 sampel kolom dan 10 sampel baris."""

cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_new['name'], columns=anime_new['name'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Kemudian, dibuat fungsi rekomendasi anime berdasarkan kemiripan dari genre dengan keluaran fungsi berupa top-N recommendation yang nilai N-nya diatur dalam parameter k."""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=anime_new[['name', 'genre']], k=10):
    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(nama_anime, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Ditahap ini, dilakukan pengetesan fungsi rekomendasi yang telah dibuat menggunakan sebuah anime bernama 'Shingeki no Kyojin' untuk menghasilkan 10 rekomendasi anime baru."""

anime_new[anime_new.name.eq('Shingeki no Kyojin')]

"""Selanjutnya, kita menyimpan 10 rekomendasi anime baru yang sama dengan 'Shingeki no Kyojin' di variabel `recs`"""

recs = anime_recommendations('Shingeki no Kyojin')
recs

"""## Evaluation

Ditahap ini, dibuat sebuah fungsi untuk menghitung nilai metriks Precision pada sistem rekomendasi, dimana akan dihitung jumlah anime yang genrenya relevan dengan jumlah anime yang direkomendasikan.
"""

def precision_at_k(query_anime_name, recommendations, anime_data):
    # Ambil genre anime acuan
    query_genres = anime_data[anime_data['name'] == query_anime_name]['genre'].values[0]
    query_genres_set = set(query_genres.lower().split(','))

    # Hitung relevansi setiap anime rekomendasi
    relevant_count = 0
    for genre_str in recommendations['genre']:
        rec_genres_set = set(genre_str.lower().split(','))
        if query_genres_set & rec_genres_set:  # set intersection tidak kosong
            relevant_count += 1

    # Precision = relevan / total rekomendasi
    precision = relevant_count / len(recommendations)
    return precision
precision = precision_at_k('Shingeki no Kyojin', recs, anime_new)
print(f'Precision: {precision:.2f}')

"""Dari output kode diatas, didapat hasil metriks precision berupa 1.00. Ini berarti semua anime yang direkomendasikan memiliki setidaknya satu genre yang sama dengan yang dimiliki oleh anime 'Shingeki no Kyojin'

# Collaborative Filtering

Model-Based Deep Learning Collaborative Filtering adalah pendekatan yang menggabungkan teknik collaborative filtering dengan metode deep learning untuk meningkatkan akurasi dan efektivitas sistem rekomendasi.

## Data Preparation

Ditahap ini, ada beberapa tahap persiapan data yang dilakukan, yaitu:
- Menghapus Kolom yang Tidak Dibutuhkan
- Encoding user_id dan anime_id
- Membagi Data untuk Training dan Validasi

### Menghapus Kolom yang Tidak Dibutuhkan

Ditahap ini, hanya data kolom `user_id`, `anime_id`, dan `rating` saja yang dibutuhkan dalam tahap pemodelan, sehingga kolom `name` dan `genre` dihapus dengan menggunakan fungsi drop().
"""

anime_ratings = df_ratings_anime.copy()
anime_ratings.drop(['name', 'genre'], axis=1, inplace=True)
anime_ratings

"""### Encoding user_id dan anime_id

Pertama, dilakukan encoding terhadap user_id
"""

user_ids = anime_ratings['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""Selanjutnya, dilakukan encoding juga terhadap anime_id"""

anime_ids = anime_ratings['anime_id'].unique().tolist()
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

"""Setelah itu, user_id dan anime_id dipetakan ke dataframe yang berkaitan"""

# Mapping userID ke dataframe user
anime_ratings['user'] = anime_ratings['user_id'].map(user_to_user_encoded)

# Mapping placeID ke dataframe anime
anime_ratings['anime'] = anime_ratings['anime_id'].map(anime_to_anime_encoded)

"""Terakhir, dilakukan pengecekan beberapa hal dalam data seperti jumlah user, jumlah anime, dan kemudian mengubah nilai rating menjadi float. Dilakukan juga pengecekan terhadap rating minimum dan rating maksimum"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)

# Mengubah rating menjadi nilai float
anime_ratings['rating'] = anime_ratings['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(anime_ratings['rating'])

# Nilai maksimal rating
max_rating = max(anime_ratings['rating'])

print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""Dari output kode diatas, dapat dilihat bahwa jumlah user: 69600, jumlah anime: 9892, rating minimum: 1.0, dan rating maksimum: 10.0

### Membagi Data untuk Training dan Validasi

Ditahap ini, dilakukan pembagian data training dan data validasi untuk proses pelatihan model. Pertama-tama, dilakukan pengacakan data supaya distribusinya menjadi acak.
"""

anime_ratings = anime_ratings.sample(frac=1, random_state=42)
anime_ratings

"""Selanjutnya, data train dan validasi dibagi engan komposisi 80:20. Namun sebelumnya, dilakukan pemetaan (mapping) data user dan anime menjadi satu value terlebih dahulu yaitu x. Lalu, dibuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training dan disimpan pada variabel y."""

x = anime_ratings[['user', 'anime']].values
y = anime_ratings['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * anime_ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(f'Ukuran dataset: {x.shape}')
print(f'Ukuran data train: {x_train.shape}')
print(f'Ukuran data validasi: {x_val.shape}')

"""## Modelling

Ditahap ini, dibuat sebuah class RecommenderNet dengan keras Model class. Class ini merupakan model yang akan menghitung skor kecocokan antara pengguna dan anime dengan teknik embedding. Pertama, dilakukan proses embedding terhadap data user dan anime. Selanjutnya, dilakukan operasi perkalian dot product antara embedding user dan anime. Lalu, ditambahkan dropout untuk mengurangi overfitting. Selain itu, ditambahkan juga bias untuk setiap user dan anime. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid.
"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'uniform',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.anime_embedding = layers.Embedding(
        num_anime,
        embedding_size,
        embeddings_initializer = 'uniform',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1)
    self.dropout = layers.Dropout(0.3)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    anime_vector = self.anime_embedding(inputs[:, 1])
    anime_bias = self.anime_bias(inputs[:, 1])

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)
    dot_user_anime = self.dropout(dot_user_anime)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x)

"""Selanjutnya, dilakukan proses compile terhadap model. Model menggunakan Mean Squared Error untuk menghitung loss function, RMSprop sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation."""

model = RecommenderNet(num_users, num_anime, 10) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.MeanSquaredError(),
    optimizer = keras.optimizers.RMSprop(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Selanjutnya dilakukan proses training dengan method fit"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 512,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""Dapat dilihat dari output diatas, model memperoleh nilai root_mean_squared_error: 0.1420 untuk data latih dan 0.1422 untuk data validasi

Selanjutnya, dilakukan pengetesan model untuk mendapatkan rekomendasi resto. Pertama, diambil sampel user secara acak dan definisikan variabel anime_not_watched yang merupakan daftar anime yang belum pernah dinonton oleh pengguna. Sebelumnya, pengguna telah memberi rating pada beberapa animeyang telah mereka nonton. Rating ini akan digunakan untuk membuat rekomendasi anime yang mungkin cocok untuk pengguna.
"""

animes_df = anime_new
df = pd.read_csv('rating.csv')

user_id = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_id]

anime_not_watched = animes_df[~animes_df['anime_id'].isin(anime_watched_by_user.anime_id.values)]['anime_id']
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_watched = [[anime_to_anime_encoded.get(x)] for x in anime_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

"""Selanjutnya, untuk memperoleh rekomendasi anime, digunakan fungsi model.predict() dari library Keras dengan menerapkan kode berikut."""

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing Anime Recommendations For User: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = animes_df[animes_df['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ':', row.genre)

print('----' * 8)
print('Top 10 Anime Recommendations')
print('----' * 8)

i = 1
recommended_anime = animes_df[animes_df['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(i, ". ", row.name)
    print("   Genre: ", row.genre)
    i += 1

"""## Evaluation

Ditahap evaluasi, dilakukan visualisasi metrik
evaluasi yaitu Root Mean Squared Error (RMSE) menggunakan plot garis untuk menunjukkan perubahan RMSE pada setiap epochs.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Berdasarkan grafik tersebut, dapat dilihat bahwa nilai error akhir untuk data training sebesar 0.1420 dan error pada data validase sebesar 0.1422. Nilai tersebut cukup bagus untuk sistem rekomendasi."""